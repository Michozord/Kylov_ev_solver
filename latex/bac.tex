\documentclass[a4paper,11pt,bibliography=totoc,listof=totoc,headinclude=true,cleardoublepage=empty,oneside]{scrbook}
% Option "oneside" für einseitigen Druck. Weglassen, falls die Arbeit doppelseitig gedruckt wird

\usepackage[english,ngerman]{babel}
\usepackage[utf8]{inputenc}
%\usepackage{fullpage}
\usepackage{ifthen}
\usepackage{color}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}

% links in pdf
\usepackage[unicode,colorlinks=true,pagebackref=false]{hyperref}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\bigO}{\mathcal{O}}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%


% Zum Druck verwende schwarze Links!
%\usepackage[unicode,colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black,pagebackref=false]{hyperref} 
	% colorlinks=false umrahmt Links statt einzufaerben, 


% document style
\KOMAoptions{footinclude=false} % Fusszeile wird nicht zu Satzspiegel gezaehlt
\KOMAoptions{headsepline=true} % Trennlinie zwischen Kopfzeile und Text
\KOMAoptions{DIV=12} % beeinflusst Satzspiegel
\KOMAoptions{BCOR=8mm} % Bindekorrektur
\pagestyle{headings} % mit Kopfzeilen

\recalctypearea % berechne Satzspiegel neu

\definecolor{change}{rgb}{0,.55,.55}

\def\revision#1{{\color{red}#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITELSEITE [OBLIGATORISCH]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{Alph}
\selectlanguage{ngerman}

\begin{titlepage}
  %\vspace*{-2cm}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{TULogo.eps}
    \vskip 1cm%
    {\LARGE B~\Large A~C~H~E~L~O~R~A~R~B~E~I~T}
    \vskip 8mm
    {\huge\bfseries\color{change}Titel \\[1ex] ggf.\ mehrzeilig}
    \vskip 1cm
    \large 
    ausgef\"uhrt am    
    \vskip 0.75cm
    {\Large Institut f\"ur\\[1ex] Analysis und Scientific Computing}\\[1ex]
    {\Large TU Wien}
    \vskip0.75cm
    unter der Anleitung von
    \vskip0.75cm
    {\Large\bfseries 
Associate Prof. Dipl.-Math. Dr.rer.nat. Lothar Nannen}\\[1ex]
    \vskip 0.5cm
    durch
    \vskip 0.5cm
    {\Large\bfseries Michał Trojanowski}\\[1ex]
    Matrikelnummer: 12108865\\[1ex]
  \end{center}
  
  \vfill
  
  \small
  Wien, am \today
  \vspace*{-15mm}
\end{titlepage}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DANKSAGUNG / ACKNOWLEDGEMENT [OPTIONAL]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Danksagung} %\chapter*{Acknowledgement}
\thispagestyle{empty}
\selectlanguage{ngerman} %\selectlanguage{english}

{\color{change}
\begin{itemize}
\item auf Deutsch oder Englisch
\item Die Danksagung (engl. {\em Acknowledgement}) ist optional und kann auch entfallen. Denken Sie ggf.\ an Ihre eigenen Eltern!

\item Falls die Arbeit durch eine Forschungsprojekt finanziert wurde, so ist jedenfalls der Fördergeber (z.B.\ FWF oder WWTF) mit Projektnummer und Projektname zu nennen.
\begin{itemize}
\item siehe z.B.\ Dissertation von Michele Ruggeri:
\item[] \href{https://publik.tuwien.ac.at/files/publik_252806.pdf}{\ttfamily https://publik.tuwien.ac.at/files/publik\_252806.pdf}
\end{itemize}

\end{itemize}
}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EIDESSTATTLICHE ERKLAERUNG [OBLIGATORISCH]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Eidesstattliche Erkl\"arung}
\thispagestyle{empty}
\selectlanguage{ngerman}
\thispagestyle{empty}

\vspace*{2cm}

Ich erkl\"are an Eides statt, dass ich die vorliegende Bachelorarbeit selbstst\"andig und ohne fremde Hilfe verfasst, andere als die angegebenen Quellen und Hilfsmittel nicht benutzt bzw. die w\"ortlich oder sinngem\"a{\ss} entnommenen Stellen als solche kenntlich gemacht habe.

\vspace*{3cm}

\noindent
Wien, am \today
%
\hfill 
%
\begin{minipage}[t]{5cm}
\centering
\underline{\hspace*{5cm}}\\
\small{Michał Trojanowski}
\end{minipage}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INHALTSVERZEICHNIS [OBLIGATORISCH]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
%\selectlanguage{ngerman} %
\selectlanguage{english} 

\tableofcontents

\cleardoublepage
\pagenumbering{arabic} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EINLEITUNG / INTRODUCTION [OBLIGATORISCH]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:introduction}


\chapter{A Krylov eigenvalue solver based on filtered time domain solutions}
\label{chapter:ftd}
In this chapter we present concept of Krylov eigenvalue solver in a simple setting -- computation of eigenvalues of the negative Laplace operator. We formulate the weak form of the problem and introduce discretization in space. Then we define Krylov spaces and project our problem onto this spaces with smaller dimension. ..... This chapter is based on \cite{nannen}.
\begin{definition}[Eigenvalue problem of the negative Laplace operator]\label{def:ev problem}
    Let $\Omega~\subset~\R^d$ for $d=2,3$ be a bounded domain with Lipschitz boundary. We solve for eigenvalues $\omega^2~\in~\R_{+}$ and eigenfunctions $u\in H^1(\Omega)$ of the negative Laplace operator with Neumann boundary conditions:
        \begin{align}\begin{split}\label{eq:ev problem}
              -\Delta u &= \omega^2 u \hspace{.3cm} \text{ in } \Omega, \\
              \frac{\partial u}{\partial \nu} &= 0 \hspace{.3cm}\text{ on } \partial\Omega.
        \end{split}\end{align}
    Here $\frac{\partial}{\partial\nu}$ denotes the normal derivative and $H^1$ is the Sobolev space. 
\end{definition}
Now we discretize the problem in space and we fix the partition $\mathcal{T}$ of $\Omega$. We introduce a discrete solution space as a finite-dimensional space of piecewise polynomials on $\Omega$. 
\begin{definition}[Discrete solution space]\label{def:solution space}
    Let $\mathcal{P}_p$ denote the space of polynomials up to degree $p \in \N$. We define discrete solution space as  
    \begin{equation*}
        V_h := \{v \in H^1(\Omega): \hspace{.3cm} \forall T \in \mathcal{T} \, v|_T \in \mathcal{P}_p \}
    \end{equation*}
    with finite dimension $N := \mathrm{dim} (V_h)$. 
\end{definition}

Using Gauss theorem, we can formulate the weak and discrete form of the problem \eqref{eq:ev problem}.
\begin{definition}[Weak formulation of eigenvalue problem]\label{def:weak form}
    Let $\Omega$ be a bounded Lipschitz domain, like in Definition \ref{def:ev problem}, and $V_h$ be the discrete solution space. We solve for eigenvalues $\omega_h^2 \in \R_+$ corresponding to discrete eigenfunctions $u \in V_h$, such that for all test functions $\varphi \in V_h$ holds:
    \begin{equation}\label{eq:weak form}
        \int_\Omega \nabla u \cdot \nabla \varphi \, dx = \omega_h^2 \int_\Omega u \varphi \, dx.
    \end{equation}  
\end{definition}
To simplify the notation in the subsequent part, we neglect the use of index $h$ for discrete eigenfunctions and eigenvalues. Now we show, that the discrete eigenvalue problem \eqref{eq:weak form} is in fact just an eigenvalue problem for a matrix, since our solutions space is finite-dimensional. 
\begin{definition}\label{def:SM matrices}
    Let $V_h$ be $N$-dimensional solution space with basis $(\varphi_1, \dots, \varphi_N)$. We define matrices $S:=(s_{ij})_{i, j=1}^N $ and $M:=(m_{ij})_{i, j=1}^N $ with:
    \begin{equation*}
        s_{ij} := \int_\Omega \nabla \varphi_i \cdot \nabla \varphi_j \, dx \hspace{.7cm} \text{ and } \hspace{.7cm} m_{ij} := \int_\Omega \varphi_i \varphi_j \, dx.
    \end{equation*}
\end{definition}
\begin{lemma}
    Equivalent to the problem Definition \ref{def:weak form} is eigenvalue problem to find non-trivial $v \in \R^N$ and $\omega^2 \in \R_+$, such that:
    \begin{equation}\label{eq:matrix form}
        Sv = \omega^2 Mv
    \end{equation}
    with matrices $S, M \in \R^{N \times N}$ from Definition \ref{def:SM matrices}.
\end{lemma}
\begin{proof}
    Since $(\varphi_1, \dots, \varphi_N)$ is a basis of $V_h$, it is sufficient, if \eqref{eq:weak form} holds for all basis functions. With sufficient $v\in \R^N$ we can also replace $u = (\varphi_1, \dots, \varphi_N) v$. We obtain:
    \begin{align*}
        \forall j=1, \dots, N : \, \int_\Omega (\nabla\varphi_1, \dots, \nabla\varphi_N)v\cdot \nabla \varphi_j \, dx &= \omega^2 \int_\Omega (\varphi_1, \dots, \varphi_N)v \varphi_j \, dx, \\
        \forall j=1, \dots, N : \, \int_\Omega (\nabla \varphi_j \cdot \nabla \varphi_1, \dots, \nabla \varphi_j \cdot \nabla \varphi_N) v \, dx &= \omega^2 \int_\Omega (\varphi_j\varphi_1, \dots, \varphi_j\varphi_N)v \,dx, \\ 
        \forall j = 1, \dots, N: \, (s_{j1},\dots,s_{jN})v &= \omega^2 (m_{j1},\dots, m_{jN})v,
    \end{align*}
    what is equivalent to \eqref{eq:matrix form}.
\end{proof}
\begin{rem}
    Matrices $S$ and $M$ from Definition \ref{def:SM matrices} are self-adjoint. Furthermore matrix $S$ is positive semi-definite and $M$ is positive-definite.
\end{rem}




\section{Elementary eigenvalue solvers for matrices}
Since we have reformulated eigenvalue problem of the negative Laplacian operator (see Definition \ref{def:ev problem}) into an eigenvalue problem for matrices, we recall elementary numerical algorithms to solve such problems. For proofs of convergence of those algorithms, we refer to \cite{numericsAB}. 

First algorithm provides approximation of an eigenvector to eigenvalue with largest absolute value among eigenvalues of a matrix. 

\begin{algorithm}[H]
\caption{Power iteration}\label{alg:power iteration}
\begin{algorithmic}
    \State \textbf{Input:} $A \in \R^{N \times N}$, start vector $v^{(0)}\in \R^N$
    \For{$i = 1, 2, \dots$}
        \State $v \gets Av^{(i-1)} $
        \State $v^{(i)} \gets \frac{v}{\|v\|_2}$
    \EndFor
    \State \textbf{Output:} $v^{(i)}$ -- approximation of an eigenvector to eigenvalue with largest absolute value.
    \end{algorithmic}
\end{algorithm}
\begin{theorem}
    Let $A \in \R^{N \times N}$ be a diagonalizable matrix with eigenvalues $\mu_1, \dots \mu_N$, such that $|\mu_1| > |\mu_2| \geq |\mu_j|$ for all $j  = 3,\dots,N$. Let $(v_1, \dots, v_N)$ denote a basis of $\R^N$, such that for all $j=1, \dots N$ $v_j$ is a normed eigenvector to eigenvalue $\mu_j$. Let $v = \sum_{j=1}^N c_j v_j \in \R^N$ be a start vector, such that $c_1 \neq 0$. Then for all $i \in \N$ holds error estimation for eigenspace:
    \begin{equation*}
        \left\| v^{(i)} - \frac{\mu_1^i c_1}{|\mu_1^i c_1|} v_1 \right\|_2 = \bigO\left( \left|\frac{\mu_2}{\mu_1}\right|^i\right) \text{ for } i \rightarrow \infty.
    \end{equation*}
    Furthermore holds error estimation for convergence of eigenvalue. Let $\mu^{(i)} := (Ax^{(i)})_k / x^{(i)}_k $ for some $k = 1, \dots, N$. It holds:
    \begin{equation*}
        |\mu_1 - \mu^{(i)}| = \bigO\left( \left|\frac{\mu_2}{\mu_1}\right|^i\right) \text{ for } i \rightarrow \infty.
    \end{equation*}
\end{theorem}
\begin{proof}
    We refer to \cite[p. 116]{numericsAB}.
\end{proof}

Second algorithm allows us to compute whole spectrum of a matrix. 
\begin{algorithm}[H]
\caption{QR algorithm}\label{alg:QR alg}
\begin{algorithmic}
    \State \textbf{Input:} $A \in \R^{N \times N}$
    \State $A^{(0)} \gets A$
    \For{$i = 1, 2, \dots$}
        \State compute QR-decomposition: $Q^{(i)} R^{(i)} = A^{(i-1)}$ 
        \State $A^{(i)} \gets R^{(i)}Q^{(i)}$
    \EndFor
    \State \textbf{Output:} $A^{(i)}$ -- approximation of upper triangular matrix with eigenvalues of $A$ on diagonal. 
    \end{algorithmic}
\end{algorithm}
\begin{theorem}
    Let $A \in \R^{N \times N}$ be a diagonalizable matrix with pairwise different absolute values of eigenvalues: $\mu_1 > \mu_2 > \dots > \mu_N$. Let $\Lambda^{(i)} := \left(A^{(i)}_{11}, \dots A^{(i)}_{NN}\right)$. Then $\Lambda^{(i)}$ converges linear towards $(\mu_1, \dots, \mu_N)$ for $i\rightarrow\infty$. 
\end{theorem}
\begin{proof}
    We refer to \cite[p. 120]{numericsAB}.
\end{proof}

Since our goal is to find eigenvalues of the negative Laplace operator, we could use the QR algorithm to obtain all eigenvalues of the discrete problem. However it is not possible due to high computation costs of QR algorithm. We focus on situations, where the problem on the discrete level is high-dimensional. QR algorithm demand computation of a QR-decomposition of a $N \times N$ matrix, what has cubic computational complexity. Therefore is this method for our problem out of reach. We have to deploy a method, that is based on direct solver, such as power iteration. 

\section{Krylov eigenvalue solver}
Now we need an assumption, that there exists matrix $C\in \R^{N \times N}$, such that its eigenvectors are exact eigenvectors of the discrete problem \eqref{eq:matrix form}. 
\begin{prop}\label{prop:C}
    Let $C \in \R^{N\times N}$ be a matrix, such that there exists eigenvalue $\mu \in \R$ to the eigenvector $w\in \R^N$, i.e., $Cw = \mu w$, if and only if $w$ is an eigenvector or linear combination of eigenvectors in the discrete problem \eqref{eq:matrix form} to some eigenvalue $\omega^2$.
\end{prop}
We obtain properties of the matrix $C$, that will be needed. Exact choice and construction of this matrix will be discussed later. We recall the definition of a Krylov space. 
\begin{definition}[Krylov space]
    Let $C \in \R^{N \times N}$ be a matrix and $r\in \R^N$ be a normalized start vector, i.e. $\|r\|=1$. For $m\in \N$ we define Krylov space as a subspace of $\R^N$:
    \begin{equation*}
        \mathcal{K}_m(C; r) := \mathrm{span}\left\{r, Cr, \dots, C^{m-1}r\right\}.
    \end{equation*}
\end{definition}
In this thesis we focus on situations, when $N$ is large. Our goal is to project $N$-dimensional eigenvalue problem \eqref{eq:matrix form} onto $m$-dimensional Krylov space, using orthonormal basis of this space. Typically we choose $m$ much smaller than $N$, so that this problem is solvable with low computational costs with direct solver. 
\begin{prop}
    Let $\mathcal{K}_m(C; r)$ be a $m$-dimensional Krylov space to matrix $C\in \R^{N\times N}$ and start vector $r\in \R^N$, $\|r\|=1$. We obtain an orthonormal basis $(b_0, \dots, b_{m-1})$ of Krylov space with Gram-Schmidt orthonormalization:
    \begin{equation*}
        b_0 := r, \text{ and } \widetilde{b}_{j} := Cb_{j-1} - \sum_{i=0}^{j-1} (b_i^T C b_{j-1}) b_i , \, b_j := \frac{\Tilde{b}_{j-1}}{\|\Tilde{b}_{j-1}\|_2} \text{ for all } j=1, \dots, m-1. 
    \end{equation*}
\end{prop}
Now we can project the original problem \eqref{eq:matrix form} onto $m$-dimensional Krylov space. 
\begin{prop}[Eigenvalue problem on Krylov space]
    Let $B_m = (b_0, \dots, b_{m-1}) \in \R^{N\times m}$ be an orthonormal basis of a $m$-dimensional Krylov space $\mathcal{K}_m(C; r)$. Eigenvalue problem on Krylov space is to find eigenvalues $\omega_m^2 \in \R_+$ and eigenvectors $v_m\in\R^m$, such that:
    \begin{equation}\label{eq:Krylov problem}
        B_m^T S B_m v_m = \omega_m^2 B_m^T M B_m v_m.
    \end{equation}
\end{prop}

Since matrices $S$ and $M$ are Hermitian, Krylov iteration leads to convergence of eigenspaces of \eqref{eq:Krylov problem} towards eigenspace of $C$ corresponding to eigenvalue $\mu_{max}$ with largest absolute value. Obviously, projected eigenvalues $\omega_m^2$ converge towards eigenvalue $\omega^2$ of original problem \eqref{eq:matrix form} to eigenspace corresponding to $\mu_{max}$.

To sum up core idea of this section, we can explicit formulate an (not implementable yet) algorithm based on Krylov spaces to compute eigenvalues $\omega^2$.
\begin{algorithm}[H]
\caption{Krylov eigenvalue solver}\
    \begin{algorithmic}
        \State \textbf{Input:} matrix $C$, start vector $r$, $M^{-1}$, $S$
        \State $b_0 \gets r$
        \State $m \gets 1$ \Comment{Dimension of Krylov space}
        \Do 
            \State $b_{m} \gets Cb_{m-1} - \sum_{i=0}^{m-1} (b_i^T C b_{m-1}) b_i$ \Comment{Gram-Schmidt orthonormalisation}
            \State $ b_m \gets b_{m}/\|b_{m}\|_2 $ 
            \State $m \gets m+1$
            \State $B_m \gets (b_0, \dots, b_{m-1})$ \Comment{Projection matrix}
            \State $A \gets B_m^T M^{-1}S B_m$
            \State solve $Av = \omega_m^2 v$ with power iteration
        \doWhile{stopping criterion}
    \end{algorithmic}
\end{algorithm}


Our goal is to compute eigenvalues $\omega^2$ of \eqref{eq:matrix form} in a given region of interest $(\omega_{\min}^2, \omega_{\max}^2)$. Therefore we need, that matrix $C$ fulfills following four conditions.
\begin{itemize}
    \item $C$ satisfies conditions in Proposition \ref{prop:C}.
    \item Eigenvalues $\mu$ corresponding to eigenspaces of eigenvalues $\omega^2$ in region of interest have large absolute value.
    \item Eigenvalues $\mu$ corresponding to eigenspaces of eigenvalues $\omega^2$, that are unsought, should be close 0. 
    \item Each Krylov-step $r \mapsto Cr$ can be computed with low costs.
\end{itemize}
In other words, use of matrix $C$ filters sought eigenvalues from among all eigenvalues of the original problem \eqref{eq:matrix form} via common eigenspaces. 

\begin{rem}
    Shift-and-inverse matrix:
    \begin{equation*}
        C_\rho := (S - \rho M)^{-1}M
    \end{equation*}
    for some $\rho \in \R$ has eigenvalue $\mu = (\omega^2 - \rho)^{-1}$ corresponding to eigenspace to eigenvalue $\omega^2$ of original problem \eqref{eq:matrix form}. Therefore $C_\rho$ with $\rho = (\omega^2_{\min}+\omega^2_{\max})/2 $ would be a possible choice of matrix C. Nevertheless, it requires inverse of a $N\times N$ matrix $(S - \rho M)$, what for large $N$ is impossible with low computational complexity.
\end{rem}
\begin{proof}
    Let $(\mu, w)\in \R\times\R^N$ be an eigenpair of $C_\rho$:
    \begin{align*}
        C_\rho w &= \mu w,  \\
        (S - \rho M)^{-1}Mw &= \mu w, \\
        \frac{1}{\mu}Mw &= (S - \rho M)w, \\
        Sw &= \underbrace{\left(\rho - \frac{1}{\mu}\right)}_{\omega^2} Mw. 
    \end{align*}
    Therefore $\mu = (\omega^2 - \rho)^{-1}$. Obviously, $|\mu|$ takes on largest values, if $\omega^2 \approx \rho$.
\end{proof}

\section{Filtered time-domain solutions}
To construct appropriate finite-dimensional operator, that will replace role of the matrix $C$ from previous section, we proceed as follows. We consider homogeneous wave equation with some initial condition $u_0 = (\varphi_1, \dots, \varphi_N)r$ projected onto our discrete solution space $V_h$ (see Definition \ref{def:solution space}). We discretize the problem in space and formulate its weak form. The evaluation of our operator at the point $r$ is a time integral of the solution to this semi-discrete problem.

We consider homogeneous 2- or 3-dimensional wave equation in $\Omega \times (0, \infty)$:
\begin{align}
\begin{split}\label{eq:wave equation}
    \ddot{u} = \Delta u \, \,\, \text{in } \Omega \times (0, \infty),& \, \,\,  u = 0 \, \,\, \text{in } \partial\Omega\times (0, \infty),\\
    \,\,\,u( \cdot, 0)= u_0, & \, \dot{u}(\cdot, 0) = 0 \, \,\, \text{in } \Omega,
\end{split}
\end{align}
where $u$ is a function $\Omega \times [0, \infty) \rightarrow \R$ and $\ddot{u}$ denotes the second time-derivative.
\begin{prop}
    Let $V_h$ be a discrete solution space with basis $(\varphi_1, \dots, \varphi_N)$ and let $r$ denote the coordinate vector of the projection of $u_0$ onto $V_h$, i.e. $u_0 = (\varphi_1, \dots, \varphi_N)r$. Let $S$ and $M$ denote the matrices from Definition \ref{def:SM matrices}. Discretization in space of the problem \eqref{eq:wave equation} leads to following linear system of ordinary differential equations:
    \begin{align}\label{eq:discr wave equation}
    \begin{split}
        M \ddot{y}(t; r) &= -S y (t; r) \,\,\, \text{ for all } t \in (0, \infty) \\
        y(0; r) &= r, \, \dot{y}(0; r) = 0. 
    \end{split}
    \end{align}
\end{prop}
\begin{proof}
    Theory of partial differential equations leads us to ansatz $u(t, x) = \sum_{i=1}^\infty \kappa_i(t) \xi_i(x)$, where $\xi_i$ are the eigenfunctions of the negative Laplace operator \cite[p. 122]{Jungel}. Since we want to solve the problem in our solution space $V_h$, we can assume, that $u(\cdot, t)=(\varphi_1, \dots, \varphi_N)y(t)$ for some vector-valued function $y: [0, \infty) \rightarrow \R^N$.

    We reformulate $\ddot{u} = \Delta u$ to its weak form:
    \begin{align*}
        \forall \varphi \in V_h : \, \int_\Omega \ddot{u} \varphi \, dx &= \int_\Omega \Delta u \varphi \, dx, \\
        \forall \varphi \in V_h : \, \int_\Omega \ddot{u} \varphi \, dx &= - \int_\Omega \nabla  u\cdot  \nabla \varphi \, dx.
    \end{align*}
    $V_h$ is a finite-dimensional space, therefore condition "for all test functions" is equivalent to condition "for all basis test functions":
    \begin{align*}
        \forall j=1, \dots, N:\, \int_\Omega (\varphi_1, \dots, \varphi_N)\ddot{y} \varphi_j \, dx &= -  \int_\Omega (\nabla \varphi_1, \dots, \nabla \varphi_N)y\cdot  \nabla \varphi_j \, dx, \\
        M\ddot{y} &= -Sy.
    \end{align*}
    Remains to show the equivalence of initial conditions. For $u_0$ holds:
    \begin{equation*}
        (\varphi_1, \dots, \varphi_N)r = u_0 = u(\cdot, 0) = (\varphi_1, \dots, \varphi_N)y(0),
    \end{equation*}
    so $y(0) = r$ in $\Omega$ and similarly $\dot{y}(0) = 0$ in $\Omega$.
\end{proof}
Now we can solve semi-discrete wave equation \eqref{eq:discr wave equation} to construct an integral operator. 
\begin{lemma}\label{lemma:y solution}
    The unique solution to \eqref{eq:discr wave equation} is:
    \begin{equation}\label{eq:solution wave eq}
        y(t; r) = \sum_{j=1}^N \cos(\omega_j t) (v_j^T r) v_j,
    \end{equation}
    where $(v_j)_{j=1}^N$ is an orthonormal basis of $\R^N$ of eigenvectors of matrix $M^{-1}S$ with corresponding eigenvalues $\omega_j^2$.
\end{lemma}
\begin{proof}
    Because of symmetry of matrices $S$ and $M$, there exists an orthonormal basis $(v_j)_{j=1}^N$ of eigenvectors of $M^{-1}S$. We denote corresponding eigenvalues with $\omega_j^2$, $j=1, \dots N$. Obviously, for all $j=1, \dots, N$ and for all $c_{1j}, c_{2j}\in \R$ function $t\mapsto \left(c_{1j} \cos(\omega_j t) + c_{2j} \sin(\omega_j t)\right)v_j$ solves the differential equation \eqref{eq:discr wave equation}. Due to linearity of the problem, 
    \begin{equation*}
        y(t) = \sum_{j=1}^N \left(c_{1j} \cos(\omega_j t) + c_{2j} \sin(\omega_j t)\right)v_j
    \end{equation*}
    solves the differential equation. Initial values lead to the form \eqref{eq:solution wave eq}. Uniqueness of the solution follows from the Picard-Lindelöf theorem.
\end{proof}

Now we can define an integral operator, that maps initial value of the semi-discrete wave equation \eqref{eq:discr wave equation} to a weighted time-integral of its unique solution. Discrete form of this operator will take over the role of matrix $C$. Depending on choice of the weight function, eigenvalues of this discrete operator may fulfill requirements, that we have set for matrix $C$ in previous section. This will be discussed later in Chapter \ref{chapter:function}. 

\begin{definition}
    Let $\alpha: [0; \infty) \rightarrow \R$ be a piecewise continuous function with compact support. We define $\Pi_\alpha: \R^N \rightarrow \R^N$ as linear operator:
    \begin{equation*}
        \Pi_\alpha r := \int_0^\infty \alpha(t) y(t;r) \,dt,
    \end{equation*}
    where $y(t;r)$ is the unique solution to \eqref{eq:discr wave equation} from Lemma \ref{lemma:y solution}.
\end{definition}

Following lemma determines correspondence between eigenvalues of $\Pi_\alpha$ and eigenvalues of the original problem \eqref{eq:matrix form}.
\begin{lemma}
    Let $\beta_\alpha : [0, \infty) \rightarrow \R$ be filter function defined by:
    \begin{equation}
        \beta_\alpha(\omega) := \int_0^\infty \alpha(t) \cos(\omega t) \,dt. 
    \end{equation}
    Then hold following two statements.
    \begin{enumerate}
        \item If $\omega^2$ is an eigenvalue of \eqref{eq:discr wave equation} to eigenvector $v$, then $v$ is also an eigenvector of $\Pi_\alpha$ corresponding to eigenvalue $\beta_\alpha(\omega)$. 
        \item If $\lambda$ is an eigenvalue of $\Pi_\alpha$ corresponding to eigenvector $v$, then there exists eigenvalue $\omega^2$ of \eqref{eq:discr wave equation}, such that $\beta_\alpha (\omega) = \lambda$ and 
        \begin{equation*}
            v \in \bigoplus\{ U : \exists \omega \, U \text{ is eigenspace to eigenvalue to } \omega \text{ and } \beta_\alpha(\omega) = \lambda\}.
        \end{equation*}
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item If $v$ is an eigenvector of \eqref{eq:matrix form}, then it is equal $v_j$ from \eqref{eq:solution wave eq} for some $j=1, \dots, N$. Therefore $y(t, v) = \cos(\omega_j t)v$ and 
        \begin{equation*}
            \Pi_\alpha v = \int_0^\infty \alpha(t) \cos(\omega_j t) v \, dt = \beta_\alpha(\omega) v.
        \end{equation*}
        So the first claim holds.
        \item If $\lambda$ is an eigenvalue of $\Pi_\alpha$ to eigenvector $v$, then for all $j=1, \dots, N$ holds:
        \begin{align*}
            \forall k =1, \dots, N:\, 0 &= (\Pi_\alpha v - \lambda v)v_k = \int_0^\infty \alpha(t) \sum_{j=1}^N \cos(\omega_k t) (v_j^T v)\underbrace{(v_j^T v_k)}_{=\delta_{jk}} \, dt - \lambda v^Tv_k \\ &= \left(\int_0^\infty \alpha(t) \cos(\omega_kt) - \lambda \right)v_k^T v = (\beta_\alpha(\omega_k) - \lambda)v_k^T v.
        \end{align*}
        If $\beta_\alpha (\omega_k)\neq \lambda$, then $v_k^Tv = 0$, because $(v_k)_{k=1}^N$ is a basis. Therefore $v$ is in sum of eigenspaces to eigenvalues such that $\beta_\alpha(\omega_k) = \lambda)$. Moreover $v$ is an eigenvector, so $v\neq 0$. Therefore there exists at least one $k$, such that $v_k^T v \neq 0$, what implies $\beta_\alpha (\omega_k) = \lambda$. 
    \end{enumerate}
\end{proof}




\chapter{Choice of the weight function}
\label{chapter:function}
\cite{nannen}\cite{numodes}

\bibliographystyle{alpha} 
%\bibliographystyle{abbrv}
\bibliography{literature.bib}

\end{document}
